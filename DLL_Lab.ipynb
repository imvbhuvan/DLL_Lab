{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#P3\n",
        "https://github.com/rizafahmi/pytorch-demo-basic/blob/master/11train.py"
      ],
      "metadata": {
        "id": "r7Dnbyys9BRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the target variable for multi-class classification\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = torch.from_numpy(scaler.fit_transform(X_train)).float()\n",
        "X_test = torch.from_numpy(scaler.transform(X_test)).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Define the neural network architecture\n",
        "class IrisModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(X_train.shape[1], 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 3)  # Multi-class classification with 3 classes\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.softmax(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "model = IrisModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model with Gradient Descent\n",
        "print(\"Training with Gradient Descent...\")\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, torch.max(y_train, 1)[1])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch + 1}/{10}, Loss: {loss.item()}')\n",
        "\n",
        "# Train the model with Stochastic Gradient Descent (SGD)\n",
        "print(\"\\nTraining with Stochastic Gradient Descent (SGD)...\")\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, torch.max(y_train, 1)[1])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch + 1}/{50}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "ZtXPZ4wHa7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P4\n",
        "https://github.com/PacktPublishing/Deep-learning-with-PyTorch-video/blob/master/3.cnn.for.digits.classification.ipynb"
      ],
      "metadata": {
        "id": "d3XXAO_a9Na1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = self.pool3(torch.relu(self.conv3(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "PuZshvO4bChC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P5\n",
        "https://www.geeksforgeeks.org/time-series-forecasting-using-pytorch/"
      ],
      "metadata": {
        "id": "qG3Ai02C9Xvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler  # Add this line\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rest of your code...\n",
        "\n",
        "\n",
        "# Function to fetch historical stock data\n",
        "def fetch_stock_data(ticker, start_date, end_date):\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return stock_data\n",
        "\n",
        "# Function to preprocess stock data\n",
        "def preprocess_data(stock_data):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Extracting the closing prices\n",
        "    closing_prices = stock_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "    # Scaling the closing prices\n",
        "    scaled_prices = scaler.fit_transform(closing_prices)\n",
        "\n",
        "    return scaled_prices, scaler\n",
        "\n",
        "# Function to create sequences for training the GRU model\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        seq = data[i:i+sequence_length]\n",
        "        label = data[i+sequence_length:i+sequence_length+1]\n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "    return torch.Tensor(sequences), torch.Tensor(labels)\n",
        "\n",
        "# GRU model definition\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, h_n = self.gru(x)\n",
        "        out = self.fc(h_n[-1])\n",
        "        return out\n",
        "\n",
        "# Function to plot actual vs predicted prices\n",
        "def plot_predictions(actual, predicted, ticker):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(actual, label='Actual Prices', color='blue')\n",
        "    plt.plot(predicted, label='Predicted Prices', color='red')\n",
        "    plt.title(f'{ticker} Stock Price Prediction')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    ticker = 'AAPL'\n",
        "    start_date = '2022-01-01'\n",
        "    end_date = '2023-01-01'\n",
        "    sequence_length = 10  # Number of previous days to consider for prediction\n",
        "\n",
        "    # Fetch historical stock data\n",
        "    stock_data = fetch_stock_data(ticker, start_date, end_date)\n",
        "\n",
        "    # Preprocess the data\n",
        "    scaled_prices, scaler = preprocess_data(stock_data)\n",
        "\n",
        "    # Create sequences for training the model\n",
        "    X, y = create_sequences(scaled_prices, sequence_length)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    split_ratio = 0.8\n",
        "    split_index = int(len(X) * split_ratio)\n",
        "    X_train, X_test = X[:split_index], X[split_index:]\n",
        "    y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "    # Define PyTorch DataLoader for training and testing sets\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Model parameters\n",
        "    input_size = 1\n",
        "    hidden_size = 50\n",
        "    output_size = 1\n",
        "\n",
        "    # Build and train the GRU model\n",
        "    model = GRUModel(input_size, hidden_size, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Predict on the test set\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predictions.append(outputs.numpy())\n",
        "\n",
        "    predicted_prices = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
        "    actual_prices = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
        "\n",
        "    # Plot the results\n",
        "    plot_predictions(actual_prices, predicted_prices, ticker)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-8fzQs1TbEys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P 1"
      ],
      "metadata": {
        "id": "W_-a2zwb2nw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 5  # You can adjust this based on your problem\n",
        "output_size = 3  # Number of classes in the Iris dataset\n",
        "\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    y_pred = model(X_test)\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted.numpy())\n",
        "print(f'Accuracy on the test set: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORkFbWGe0HPF",
        "outputId": "7d740617-5c7a-46e6-f7b7-0224d495c4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.3645\n",
            "Epoch [200/1000], Loss: 0.1520\n",
            "Epoch [300/1000], Loss: 0.0903\n",
            "Epoch [400/1000], Loss: 0.0704\n",
            "Epoch [500/1000], Loss: 0.0614\n",
            "Epoch [600/1000], Loss: 0.0565\n",
            "Epoch [700/1000], Loss: 0.0535\n",
            "Epoch [800/1000], Loss: 0.0516\n",
            "Epoch [900/1000], Loss: 0.0504\n",
            "Epoch [1000/1000], Loss: 0.0495\n",
            "Accuracy on the test set: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P 2"
      ],
      "metadata": {
        "id": "12anRJH521J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(X)\n",
        "y_tensor = torch.LongTensor(y)  # For multi-class classification\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for training and testing sets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "class DeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(DeepNeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.activation1 = nn.Tanh()  # Tanh activation function for the first hidden layer\n",
        "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.activation2 = nn.ReLU()  # ReLU activation function for the second hidden layer\n",
        "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
        "        self.output_activation = nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.activation2(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = X.shape[1]\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 32\n",
        "output_size = len(iris.target_names)  # Number of classes in the Iris dataset\n",
        "\n",
        "model = DeepNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is suitable for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2%}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzpVM45l0Y8p",
        "outputId": "8dcb1c64-c5ae-4854-8c3e-17362528995d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.0674, Accuracy: 70.00%\n",
            "Epoch [2/50], Loss: 1.0381, Accuracy: 73.33%\n",
            "Epoch [3/50], Loss: 0.9907, Accuracy: 76.67%\n",
            "Epoch [4/50], Loss: 0.9631, Accuracy: 76.67%\n",
            "Epoch [5/50], Loss: 0.8943, Accuracy: 76.67%\n",
            "Epoch [6/50], Loss: 0.8945, Accuracy: 76.67%\n",
            "Epoch [7/50], Loss: 0.8068, Accuracy: 80.00%\n",
            "Epoch [8/50], Loss: 0.8746, Accuracy: 80.00%\n",
            "Epoch [9/50], Loss: 0.8454, Accuracy: 76.67%\n",
            "Epoch [10/50], Loss: 0.7906, Accuracy: 76.67%\n",
            "Epoch [11/50], Loss: 0.7609, Accuracy: 80.00%\n",
            "Epoch [12/50], Loss: 0.7934, Accuracy: 80.00%\n",
            "Epoch [13/50], Loss: 0.8349, Accuracy: 80.00%\n",
            "Epoch [14/50], Loss: 0.7516, Accuracy: 80.00%\n",
            "Epoch [15/50], Loss: 0.7523, Accuracy: 83.33%\n",
            "Epoch [16/50], Loss: 0.7861, Accuracy: 83.33%\n",
            "Epoch [17/50], Loss: 0.7401, Accuracy: 86.67%\n",
            "Epoch [18/50], Loss: 0.7926, Accuracy: 86.67%\n",
            "Epoch [19/50], Loss: 0.7431, Accuracy: 86.67%\n",
            "Epoch [20/50], Loss: 0.7925, Accuracy: 93.33%\n",
            "Epoch [21/50], Loss: 0.6941, Accuracy: 93.33%\n",
            "Epoch [22/50], Loss: 0.7281, Accuracy: 93.33%\n",
            "Epoch [23/50], Loss: 0.6648, Accuracy: 93.33%\n",
            "Epoch [24/50], Loss: 0.6800, Accuracy: 96.67%\n",
            "Epoch [25/50], Loss: 0.6544, Accuracy: 100.00%\n",
            "Epoch [26/50], Loss: 0.6215, Accuracy: 100.00%\n",
            "Epoch [27/50], Loss: 0.6996, Accuracy: 100.00%\n",
            "Epoch [28/50], Loss: 0.6391, Accuracy: 100.00%\n",
            "Epoch [29/50], Loss: 0.6185, Accuracy: 100.00%\n",
            "Epoch [30/50], Loss: 0.6605, Accuracy: 100.00%\n",
            "Epoch [31/50], Loss: 0.6134, Accuracy: 100.00%\n",
            "Epoch [32/50], Loss: 0.6487, Accuracy: 100.00%\n",
            "Epoch [33/50], Loss: 0.6203, Accuracy: 100.00%\n",
            "Epoch [34/50], Loss: 0.6483, Accuracy: 100.00%\n",
            "Epoch [35/50], Loss: 0.6252, Accuracy: 100.00%\n",
            "Epoch [36/50], Loss: 0.6422, Accuracy: 100.00%\n",
            "Epoch [37/50], Loss: 0.5888, Accuracy: 100.00%\n",
            "Epoch [38/50], Loss: 0.6102, Accuracy: 100.00%\n",
            "Epoch [39/50], Loss: 0.6046, Accuracy: 100.00%\n",
            "Epoch [40/50], Loss: 0.6049, Accuracy: 100.00%\n",
            "Epoch [41/50], Loss: 0.6214, Accuracy: 100.00%\n",
            "Epoch [42/50], Loss: 0.6028, Accuracy: 96.67%\n",
            "Epoch [43/50], Loss: 0.5991, Accuracy: 96.67%\n",
            "Epoch [44/50], Loss: 0.6023, Accuracy: 96.67%\n",
            "Epoch [45/50], Loss: 0.5677, Accuracy: 96.67%\n",
            "Epoch [46/50], Loss: 0.5804, Accuracy: 100.00%\n",
            "Epoch [47/50], Loss: 0.5647, Accuracy: 100.00%\n",
            "Epoch [48/50], Loss: 0.6062, Accuracy: 100.00%\n",
            "Epoch [49/50], Loss: 0.5906, Accuracy: 100.00%\n",
            "Epoch [50/50], Loss: 0.5940, Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P 7\n"
      ],
      "metadata": {
        "id": "WaoU0JU326Mt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLjbDzJmsHdv",
        "outputId": "f5eaab0b-23cf-4bee-b693-e2ea0718c2d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90928142.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 90399771.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 153325309.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21672956.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.0012\n",
            "Epoch [2/20], Loss: 0.0010\n",
            "Epoch [3/20], Loss: 0.0009\n",
            "Epoch [4/20], Loss: 0.0009\n",
            "Epoch [5/20], Loss: 0.0009\n",
            "Epoch [6/20], Loss: 0.0009\n",
            "Epoch [7/20], Loss: 0.0008\n",
            "Epoch [8/20], Loss: 0.0008\n",
            "Epoch [9/20], Loss: 0.0008\n",
            "Epoch [10/20], Loss: 0.0008\n",
            "Epoch [11/20], Loss: 0.0008\n",
            "Epoch [12/20], Loss: 0.0008\n",
            "Epoch [13/20], Loss: 0.0008\n",
            "Epoch [14/20], Loss: 0.0008\n",
            "Epoch [15/20], Loss: 0.0008\n",
            "Epoch [16/20], Loss: 0.0008\n",
            "Epoch [17/20], Loss: 0.0008\n",
            "Epoch [18/20], Loss: 0.0008\n",
            "Epoch [19/20], Loss: 0.0008\n",
            "Epoch [20/20], Loss: 0.0008\n",
            "Reconstructed images saved as reconstructed_images.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# Define the denoising autoencoder model\n",
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 28 * 28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        decoded = decoded.view(decoded.size(0), 1, 28, 28)  # Reshape to image size\n",
        "        return encoded, decoded\n",
        "\n",
        "# Load MNIST dataset using torchvision\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + 0.3 * torch.randn_like(x))  # Add random noise\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DenoisingAutoencoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        _, reconstructed = model(data)\n",
        "        loss = criterion(reconstructed, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader.dataset):.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "torch.save(model.state_dict(), 'models/denoising_autoencoder.pth')\n",
        "\n",
        "# Test the model by reconstructing some images\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=5, shuffle=True)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        _, reconstructed = model(data)\n",
        "        original_images = data.view(-1, 1, 28, 28)\n",
        "        reconstructed_images = reconstructed.view(-1, 1, 28, 28)\n",
        "        comparison = torch.cat([original_images, reconstructed_images])\n",
        "        save_image(comparison, 'reconstructed_images.png', nrow=5)\n",
        "        break  # Only visualize the first batch\n",
        "\n",
        "print('Reconstructed images saved as reconstructed_images.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install torchvision --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j5z2qv9r4NJa",
        "outputId": "915cc201-47a6-4ed5-bebd-584d324dea35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Collecting torch\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision) (12.3.101)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "Successfully installed torchvision-0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P8"
      ],
      "metadata": {
        "id": "uWdnJkDy-JqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load IMDB dataset from Keras\n",
        "max_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_len = 200\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "x_train_tensor = torch.from_numpy(x_train).long()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "x_test_tensor = torch.from_numpy(x_test).long()\n",
        "y_test_tensor = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define LSTM Model\n",
        "class SentimentAnalysisLSTM(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, output_size):\n",
        "        super(SentimentAnalysisLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        out = self.fc(h_n[-1, :, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = max_words\n",
        "embedding_size = 128\n",
        "hidden_size = 100\n",
        "num_layers = 1\n",
        "output_size = 1\n",
        "\n",
        "model = SentimentAnalysisLSTM(input_size, embedding_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        outputs = model(batch_x)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        total += batch_y.size(0)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# Load IMDB dataset from Keras\n",
        "max_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "# Get word index\n",
        "word_index = imdb.get_word_index()\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "# Reverse the word index\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "# Sample input sentence\n",
        "sample_sentence = \"The movie was great, and I really enjoyed it!\"\n",
        "\n",
        "# Tokenize and pad the input sentence\n",
        "sample_sequence = [word_index[word] if word in word_index and word_index[word] < max_words else 0 for word in sample_sentence.split()]\n",
        "sample_sequence = sequence.pad_sequences([sample_sequence], maxlen=max_len)\n",
        "\n",
        "# Convert the sequence to a PyTorch tensor\n",
        "sample_tensor = torch.from_numpy(np.array(sample_sequence)).long()\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model(sample_tensor)\n",
        "    sentiment = \"Positive\" if prediction.item() > 0.5 else \"Negative\"\n",
        "\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"Predicted Sentiment: {sentiment} (Probability: {prediction.item():.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSdFjNzc4AVg",
        "outputId": "7d5f395e-7cc4-4b71-9a20-8e844643c849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "Epoch [1/5], Loss: 0.5635\n",
            "Epoch [2/5], Loss: 0.3502\n",
            "Epoch [3/5], Loss: 0.3741\n",
            "Epoch [4/5], Loss: 0.2994\n",
            "Epoch [5/5], Loss: 0.2035\n",
            "Test Accuracy: 0.8521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P6"
      ],
      "metadata": {
        "id": "CAroSwrqCjCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_size = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# Load IMDb dataset from Keras\n",
        "vocab_size = 20000  # Consider top 20,000 words in the dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Pad sequences to make them of equal length\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=200)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=200)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.from_numpy(X_train).long()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "X_test = torch.from_numpy(X_test).long()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output of the last time step\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMModel(vocab_size, embedding_size, hidden_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        inputs = Variable(X_train[i:i+batch_size])\n",
        "        labels = Variable(y_train[i:i+batch_size])\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, i+1, len(X_train), loss.item()))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        inputs = Variable(X_test[i:i+batch_size])\n",
        "        labels = Variable(y_test[i:i+batch_size])\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the test data: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Demonstrate text generation (generate text based on a seed input)\n",
        "seed_text = \"This movie was\"\n",
        "seed_sequence = [word_index[word] if word in word_index else 0 for word in seed_text.split()]\n",
        "seed_sequence = torch.tensor(seed_sequence).view(1, -1)\n",
        "\n",
        "# Generate text using the trained model\n",
        "generated_text = seed_text\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for _ in range(50):  # Generate 50 words\n",
        "        outputs = model(seed_sequence)\n",
        "        predicted_word_index = torch.argmax(outputs).item()\n",
        "        generated_text += \" \" + index_word[predicted_word_index]\n",
        "        seed_sequence = torch.cat((seed_sequence[:, 1:], torch.tensor([[predicted_word_index]])), dim=1)\n",
        "\n",
        "print(\"Generated Text: \", generated_text)\n"
      ],
      "metadata": {
        "id": "9AvSjFCb_7M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P7 Alternative"
      ],
      "metadata": {
        "id": "XvUXBroQbbpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the denoising autoencoder model in PyTorch\n",
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(32, 1, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset and add noise\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "def add_noise(img, noise_factor=0.5):\n",
        "    noise = torch.randn_like(img) * noise_factor\n",
        "    noisy_img = img + noise\n",
        "    return torch.clamp(noisy_img, 0., 1.)\n",
        "\n",
        "# Initialize the denoising autoencoder model\n",
        "model = DenoisingAutoencoder()\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the denoising autoencoder\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        img, _ = data\n",
        "        noisy_img = add_noise(img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(noisy_img)\n",
        "        loss = criterion(outputs, img)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Denoise some test images\n",
        "model.eval()\n",
        "denoised_images = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        img, _ = data\n",
        "        noisy_img = add_noise(img)\n",
        "        outputs = model(noisy_img)\n",
        "        denoised_images.append(outputs)\n",
        "\n",
        "# Display some examples\n",
        "n = 10  # Number of images to display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original images\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(noisy_img[i].squeeze().numpy(), cmap='gray')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display denoised images\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(denoised_images[0][i].squeeze().numpy(), cmap='gray')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jHY_W49hbfnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P2 Alternative"
      ],
      "metadata": {
        "id": "lDzSU_6pbsl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate some random data for demonstration purposes\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand((1000, 10))\n",
        "y = torch.randint(2, (1000, 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = torch.from_numpy(scaler.fit_transform(X_train)).float()\n",
        "X_test = torch.from_numpy(scaler.transform(X_test)).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Define the neural network architecture\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(X_train.shape[1], 1)\n",
        "        self.layer2 = nn.Linear(1, 64)\n",
        "        self.layer3 = nn.Linear(64, 32)\n",
        "        self.layer4 = nn.Linear(32, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.sigmoid(self.layer4(x))\n",
        "        return x\n",
        "\n",
        "model = CustomModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    test_accuracy = ((test_outputs > 0.5) == y_test.byte()).float().mean()\n",
        "\n",
        "print(f'Test Loss: {test_loss.item()}, Test Accuracy: {test_accuracy.item()}')\n"
      ],
      "metadata": {
        "id": "dPvrRRZkbvx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P1 Alternative"
      ],
      "metadata": {
        "id": "40u_jWQXbyxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate some random data for demonstration purposes\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand((1000, 10))\n",
        "y = torch.randint(2, (1000, 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = torch.from_numpy(scaler.fit_transform(X_train)).float()\n",
        "X_test = torch.from_numpy(scaler.transform(X_test)).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Define the neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.layer1 = nn.Linear(X_train.shape[1], 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.layer1(x))\n",
        "        x = self.sigmoid(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    test_accuracy = ((test_outputs > 0.5) == y_test.byte()).float().mean()\n",
        "\n",
        "print(f'Test Loss: {test_loss.item()}, Test Accuracy: {test_accuracy.item()}')\n"
      ],
      "metadata": {
        "id": "ZY_IN5rub17l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}